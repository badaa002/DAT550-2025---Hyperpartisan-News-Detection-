# ğŸ“° Hyperpartisan News Detection

## ğŸ“Œ Project Overview

This project explores the task of detecting **hyperpartisan news articles** â€” content that exhibits extreme political bias â€” using machine learning. The project is part of a university course and contributes 25% to the final grade.

We aim to:
- Compare baseline and advanced models (including deep learning)
- Evaluate model performance using both clean and noisy labels
- Analyze and reflect on the quality of labeling and feature importance

---

## ğŸ“‚ Datasets

### ğŸ”— Main Source

- **PAN @ SemEval 2019 Task 4: Hyperpartisan News Detection**  
  [Dataset on Zenodo](https://doi.org/10.5281/zenodo.1489920)  
  [Task Website](https://webis.de/data/pan-semeval-hyperpartisan-news-detection-19.html)  
  [Official Paper (Kiesel et al., 2019)](https://downloads.webis.de/publications/papers/kiesel_2019c.pdf)

### ğŸ“ Clean Labeled Data (by-article)

Used for initial training, testing, and evaluation:

- `articles-training-byarticle-20181122.xml` â€” Article texts with `<title>` and `<p>` tags
- `ground-truth-training-byarticle-20181122.xml` â€” Manually labeled `hyperpartisan: true/false`

> âœ… Crowdsourced, article-level labels with high quality (645 samples)

---

### ğŸ“ Noisy Labeled Data (by-publisher)

Used for large-scale experiments and weak-supervision training:

- `articles-training-bypublisher-20181122.xml` â€” 600,000 articles
- `ground-truth-training-bypublisher-20181122.xml` â€” Labels based on site-level political bias
- *(Optional)* `articles-validation-bypublisher-20181122.xml` â€” 150,000 validation articles
- *(Optional)* `ground-truth-validation-bypublisher-20181122.xml` â€” Labels for validation

> âš ï¸ Labels in this set are based on domain-level bias (not on article content) and may be noisy.

---

## ğŸ¯ Task Description

- **Task Type:** Supervised binary classification
- **Goal:** Predict whether a news article is *hyperpartisan* (`1`) or *not hyperpartisan* (`0`)
- **Input:** News article text (title + body)
- **Output:** Binary label
- **Challenges:**
  - Imbalanced classes in clean data
  - Noisy labeling in large dataset
  - Long-form text
  - Differentiating strong vs moderate bias

---

## ğŸ§  Approach (Planned)

### 1. **Data Preprocessing**
- Parse XML articles
- Clean and normalize text
- Match articles with correct labels
- Create structured dataset for modeling


  TODO.....

## ğŸ‘¥ Team

- **Member 1**: [Name] â€“ Data processing, baseline models, visualization
- **Member 2**: [Name] â€“ Deep learning model, evaluation, report writing
- **Member 3**: [Name] â€“ Deep learning model, evaluation, report writing

> Workload will be shared equally across all project phases.

---

## ğŸ§ª Experiments

We plan to:
- Tune hyperparameters (learning rate, dropout, regularization)
- Compare traditional ML vs deep learning
- Test multiple vectorization techniques
- Document model strengths/weaknesses

---

## ğŸ—“ï¸ Timeline

| Week | Tasks |
|------|-------|
| Mar 21â€“23 | Finalize topic, register team, set up repo |
| Mar 23â€“31 | Data exploration, define task, build baseline |
| Apr 1â€“10  | Deep learning model, embeddings |
| Apr 11â€“20 | Evaluation, error analysis, write report |
| Apr 21â€“29 | Final tuning, presentation slides |
| Apr 30    | Final submission ğŸ‰ |

---

## ğŸ“ Report & Presentation

- **Report**: 4â€“10 pages in ACM two-column LaTeX format  
- **Includes**: Dataset, methodology, evaluation, limitations, GitHub link
- **Presentation**: 5â€“10 minutes (in person preferred)

---

## ğŸ” Reproducibility

To reproduce our results:

1. Clone the repo  
2. Install dependencies with `requirements.txt`  
3. Follow the instructions in `notebooks/` or `src/`  
4. Run the main pipeline: `python main.py` *(WIP)*  
5. All models and outputs will be saved in `/models` and `/results` directories

---

## ğŸ“… Key Deadlines

- **Project Registration:** March 26, 2025
- **Final Submission:** April 30, 2025

---

Stay tuned for updates! ğŸš€
