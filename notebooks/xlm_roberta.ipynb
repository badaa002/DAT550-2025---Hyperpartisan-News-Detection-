{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 07:19:28.467806: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-05 07:19:28.735037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743830368.830204    1978 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743830368.858201    1978 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743830369.069903    1978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743830369.069949    1978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743830369.069950    1978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743830369.069952    1978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-05 07:19:29.093386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.utils import load_cleaned_data, split, compute_metrics, load_config\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, XLMRobertaConfig, Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import torch\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset as HFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 645\n",
      "Label distribution: {0: 407, 1: 238}\n",
      "\n",
      "Sample article (non-hyperpartisan):\n",
      "It's 1968 All Over Again. Almost a half-century ago, in 1968, the United States seemed to be falling apart. The Vietnam War, a bitter and close presidential election, antiwar protests, racial riots, p...\n",
      "\n",
      "Sample article (hyperpartisan):\n",
      "Kucinich: Reclaiming the money power. Money ( Image by 401(K) 2013 ) Permission Details DMCA No Pill Can Stop Tinnitus, But This 1 Weird Trick Can The walls are closing in on Congress. Terrifying wall...\n"
     ]
    }
   ],
   "source": [
    "config = load_config(\"../cfg/xlm_roberta.json\")\n",
    "# Load data from your existing code\n",
    "data = load_cleaned_data(config[\"data\"][\"preprocessed_data_path\"])\n",
    "X = data['full_text']\n",
    "y = data['label']\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Label distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nSample article (non-hyperpartisan):\")\n",
    "print(X[y == 0].iloc[0][:200] + \"...\")\n",
    "print(\"\\nSample article (hyperpartisan):\")\n",
    "print(X[y == 1].iloc[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model name and tokenizer from config\n",
    "model_name = config[\"model\"][\"base_model\"]\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set up XLM-RoBERTa configuration from config\n",
    "xlm_roberta_config = XLMRobertaConfig.from_pretrained(model_name)\n",
    "xlm_roberta_config.classifier_dropout = config[\"model\"][\"classifier_dropout\"]\n",
    "xlm_roberta_config.num_labels = config[\"model\"][\"num_labels\"]\n",
    "\n",
    "def tokenize_texts(texts, labels, tokenizer, max_length=config[\"data\"][\"max_length\"]):\n",
    "    \"\"\"Tokenize texts and prepare for model input\"\"\"\n",
    "    dataset_dict = {'text': texts.tolist(), 'label': labels.tolist()}\n",
    "    dataset = HFDataset.from_dict(dataset_dict)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    # Map tokenization function\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=['text']\n",
    "    )\n",
    "    \n",
    "    # Format for PyTorch\n",
    "    tokenized_dataset.set_format(\n",
    "        type='torch', \n",
    "        columns=['input_ids', 'attention_mask', 'label']\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Original training data distribution: {0: 326, 1: 190}\n",
      "Resampled training data distribution: {1: 326, 0: 326}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1db951bac044fc9d911e3a3e725173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d25a3caa7534beaa5410f657fafdaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tinnitussen/DAT550/DAT550-2025---Hyperpartisan-News-Detection-/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_27462/66789161.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='410' max='656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [410/656 02:25 < 01:27, 2.80 it/s, Epoch 5/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.669778</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.670954</td>\n",
       "      <td>0.783565</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.549267</td>\n",
       "      <td>0.773920</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.715789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.780529</td>\n",
       "      <td>0.775463</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.037616</td>\n",
       "      <td>0.769676</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.709677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2/5 ===\n",
      "Original training data distribution: {0: 324, 1: 192}\n",
      "Resampled training data distribution: {1: 324, 0: 324}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a5c0168459466cbf9b20b42f0284d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe188d4d5b842ba93d22d887068f6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tinnitussen/DAT550/DAT550-2025---Hyperpartisan-News-Detection-/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_27462/66789161.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='405' max='648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [405/648 02:52 < 01:44, 2.33 it/s, Epoch 5/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.534999</td>\n",
       "      <td>0.799764</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.465894</td>\n",
       "      <td>0.829885</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.778947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.716613</td>\n",
       "      <td>0.707177</td>\n",
       "      <td>0.494382</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.651852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.498870</td>\n",
       "      <td>0.826218</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.904384</td>\n",
       "      <td>0.827528</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.772277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3/5 ===\n",
      "Original training data distribution: {0: 330, 1: 186}\n",
      "Resampled training data distribution: {1: 330, 0: 330}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ea3999898d4535a861db165e3f6946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/660 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b53239e8f1457694494fcbf3127153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tinnitussen/DAT550/DAT550-2025---Hyperpartisan-News-Detection-/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_27462/66789161.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='664' max='664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [664/664 04:44, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.536349</td>\n",
       "      <td>0.728521</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.694215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.635398</td>\n",
       "      <td>0.689061</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.671533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.695423</td>\n",
       "      <td>0.685689</td>\n",
       "      <td>0.534091</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.671429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.528287</td>\n",
       "      <td>0.721279</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.700730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.474441</td>\n",
       "      <td>0.807193</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.765957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.939301</td>\n",
       "      <td>0.786464</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.754098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.527500</td>\n",
       "      <td>0.863854</td>\n",
       "      <td>0.791334</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.741573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.527500</td>\n",
       "      <td>0.916646</td>\n",
       "      <td>0.806943</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.767677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4/5 ===\n",
      "Original training data distribution: {0: 322, 1: 194}\n",
      "Resampled training data distribution: {1: 322, 0: 322}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d22db64019e4e9789e23d4f5cc61a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213030d1bbed4382ae9bfe94ccd05eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tinnitussen/DAT550/DAT550-2025---Hyperpartisan-News-Detection-/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_27462/66789161.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='648' max='648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [648/648 05:46, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.674998</td>\n",
       "      <td>0.778476</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.702703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423328</td>\n",
       "      <td>0.754278</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.675325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.533896</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.759494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.747375</td>\n",
       "      <td>0.794652</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.881310</td>\n",
       "      <td>0.833824</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.037119</td>\n",
       "      <td>0.805615</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>1.077821</td>\n",
       "      <td>0.805214</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>0.746988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>1.106031</td>\n",
       "      <td>0.816578</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 5/5 ===\n",
      "Original training data distribution: {0: 326, 1: 190}\n",
      "Resampled training data distribution: {1: 326, 0: 326}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882769fa162d4ea18413e5cccb72ee43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af91b9333bc64b6f883972054aafcff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tinnitussen/DAT550/DAT550-2025---Hyperpartisan-News-Detection-/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_27462/66789161.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='656' max='656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [656/656 04:41, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>0.731867</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.650602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.457636</td>\n",
       "      <td>0.771991</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.525558</td>\n",
       "      <td>0.766975</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.715447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.625526</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.708861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.831180</td>\n",
       "      <td>0.794367</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.741573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.809329</td>\n",
       "      <td>0.810957</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.764045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.424600</td>\n",
       "      <td>0.861584</td>\n",
       "      <td>0.800540</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.424600</td>\n",
       "      <td>0.828510</td>\n",
       "      <td>0.815201</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up device\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU.\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup from config\n",
    "n_splits = config[\"cross_validation\"][\"n_splits\"]\n",
    "kf = KFold(\n",
    "    n_splits=n_splits, \n",
    "    shuffle=config[\"cross_validation\"][\"shuffle\"], \n",
    "    random_state=config[\"cross_validation\"][\"random_state\"]\n",
    ")\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Apply class balancing if configured\n",
    "    if config[\"class_balancing\"][\"use_balancing\"]:\n",
    "        if config[\"class_balancing\"][\"method\"] == \"oversample\":\n",
    "            ros = RandomOverSampler(random_state=config[\"cross_validation\"][\"random_state\"])\n",
    "            train_indices = np.array(range(len(X_train_fold))).reshape(-1, 1)\n",
    "            train_indices_resampled, y_train_resampled = ros.fit_resample(train_indices, y_train_fold.values)\n",
    "            train_indices_resampled = train_indices_resampled.flatten()\n",
    "            X_train_resampled = X_train_fold.iloc[train_indices_resampled].reset_index(drop=True)\n",
    "            y_train_resampled = pd.Series(y_train_resampled)\n",
    "            \n",
    "            print(f\"Original training data distribution: {y_train_fold.value_counts().to_dict()}\")\n",
    "            print(f\"Resampled training data distribution: {y_train_resampled.value_counts().to_dict()}\")\n",
    "    else:\n",
    "        # No resampling\n",
    "        X_train_resampled, y_train_resampled = X_train_fold, y_train_fold\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_dataset = tokenize_texts(\n",
    "        X_train_resampled, \n",
    "        y_train_resampled, \n",
    "        tokenizer, \n",
    "        max_length=config[\"data\"][\"max_length\"]\n",
    "    )\n",
    "    val_dataset = tokenize_texts(\n",
    "        X_val_fold, \n",
    "        y_val_fold, \n",
    "        tokenizer, \n",
    "        max_length=config[\"data\"][\"max_length\"]\n",
    "    )\n",
    "    \n",
    "    # Create a fresh model for this fold\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=xlm_roberta_config\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Training arguments from config\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{config['training']['output_dir']}/xlm_roberta_fold_{fold+1}\",\n",
    "        learning_rate=config[\"training\"][\"learning_rate\"],\n",
    "        per_device_train_batch_size=config[\"training\"][\"batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"training\"][\"eval_batch_size\"],\n",
    "        num_train_epochs=config[\"training\"][\"epochs\"],\n",
    "        weight_decay=config[\"training\"][\"weight_decay\"],\n",
    "        evaluation_strategy=config[\"training\"][\"evaluation_strategy\"],\n",
    "        save_strategy=config[\"training\"][\"save_strategy\"],\n",
    "        load_best_model_at_end=config[\"training\"][\"load_best_model_at_end\"],\n",
    "        metric_for_best_model=config[\"training\"][\"metric_for_best_model\"],\n",
    "        save_total_limit=config[\"training\"][\"save_total_limit\"],\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=config[\"training\"][\"early_stopping_patience\"])]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    results = trainer.evaluate()\n",
    "    fold_results.append(results)\n",
    "    \n",
    "    # Save model\n",
    "    trainer.save_model(f\"{training_args.output_dir}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'balanced_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Analyze cross-validation results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m accuracies = [\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbalanced_accuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m fold_results]\n\u001b[32m      3\u001b[39m precisions = [result[\u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m fold_results]\n\u001b[32m      4\u001b[39m recalls = [result[\u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m fold_results]\n",
      "\u001b[31mKeyError\u001b[39m: 'balanced_accuracy'"
     ]
    }
   ],
   "source": [
    "# Analyze cross-validation results\n",
    "accuracies = [result['balanced_accuracy'] for result in fold_results]\n",
    "precisions = [result['precision'] for result in fold_results]\n",
    "recalls = [result['recall'] for result in fold_results]\n",
    "f1_scores = [result['f1'] for result in fold_results]\n",
    "\n",
    "print(\"\\nCross-validation summary:\")\n",
    "print(f\"Balanced accuracy: {np.mean(accuracies):.4f} Â± {np.std(accuracies):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f} Â± {np.std(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f} Â± {np.std(recalls):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}\")\n",
    "\n",
    "# Visualize metrics across folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "fold_nums = list(range(1, n_splits + 1))\n",
    "plt.plot(fold_nums, accuracies, 'o-', label='Balanced accuracy')\n",
    "plt.plot(fold_nums, precisions, 'o-', label='Precision')\n",
    "plt.plot(fold_nums, recalls, 'o-', label='Recall')\n",
    "plt.plot(fold_nums, f1_scores, 'o-', label='F1 Score')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Across Folds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model using the original train/test split\n",
    "X_train, X_test, y_train, y_test = split(X, y)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = tokenize_texts(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    tokenizer, \n",
    "    max_length=config[\"data\"][\"max_length\"]\n",
    ")\n",
    "test_dataset = tokenize_texts(\n",
    "    X_test, \n",
    "    y_test, \n",
    "    tokenizer, \n",
    "    max_length=config[\"data\"][\"max_length\"]\n",
    ")\n",
    "\n",
    "# Create final model\n",
    "final_model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=xlm_roberta_config\n",
    ").to(device)\n",
    "\n",
    "# Training arguments for final model\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=f\"{config['training']['output_dir']}/xlm_roberta_final\",\n",
    "    learning_rate=config[\"training\"][\"learning_rate\"],\n",
    "    per_device_train_batch_size=config[\"training\"][\"batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"training\"][\"eval_batch_size\"],\n",
    "    num_train_epochs=config[\"training\"][\"epochs\"],\n",
    "    weight_decay=config[\"training\"][\"weight_decay\"],\n",
    "    evaluation_strategy=config[\"training\"][\"evaluation_strategy\"],\n",
    "    save_strategy=config[\"training\"][\"save_strategy\"],\n",
    "    load_best_model_at_end=config[\"training\"][\"load_best_model_at_end\"],\n",
    "    metric_for_best_model=config[\"training\"][\"metric_for_best_model\"],\n",
    "    save_total_limit=config[\"training\"][\"save_total_limit\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=config[\"training\"][\"early_stopping_patience\"])]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "final_trainer.train()\n",
    "\n",
    "# Evaluate and save the model\n",
    "test_results = final_trainer.evaluate()\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(test_results)\n",
    "\n",
    "# Save the final model\n",
    "final_trainer.save_model(final_training_args.output_dir)\n",
    "tokenizer.save_pretrained(final_training_args.output_dir)# Train final model using the original train/test split\n",
    "X_train, X_test, y_train, y_test = split(X, y)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = tokenize_texts(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    tokenizer, \n",
    "    max_length=config[\"data\"][\"max_length\"]\n",
    ")\n",
    "test_dataset = tokenize_texts(\n",
    "    X_test, \n",
    "    y_test, \n",
    "    tokenizer, \n",
    "    max_length=config[\"data\"][\"max_length\"]\n",
    ")\n",
    "\n",
    "# Create final model\n",
    "final_model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=xlm_roberta_config\n",
    ").to(device)\n",
    "\n",
    "# Training arguments for final model\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=f\"{config['training']['output_dir']}/xlm_roberta_final\",\n",
    "    learning_rate=config[\"training\"][\"learning_rate\"],\n",
    "    per_device_train_batch_size=config[\"training\"][\"batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"training\"][\"eval_batch_size\"],\n",
    "    num_train_epochs=config[\"training\"][\"epochs\"],\n",
    "    weight_decay=config[\"training\"][\"weight_decay\"],\n",
    "    evaluation_strategy=config[\"training\"][\"evaluation_strategy\"],\n",
    "    save_strategy=config[\"training\"][\"save_strategy\"],\n",
    "    load_best_model_at_end=config[\"training\"][\"load_best_model_at_end\"],\n",
    "    metric_for_best_model=config[\"training\"][\"metric_for_best_model\"],\n",
    "    save_total_limit=config[\"training\"][\"save_total_limit\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=config[\"training\"][\"early_stopping_patience\"])]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "final_trainer.train()\n",
    "\n",
    "# Evaluate and save the model\n",
    "test_results = final_trainer.evaluate()\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(test_results)\n",
    "\n",
    "# Save the final model\n",
    "final_trainer.save_model(final_training_args.output_dir)\n",
    "tokenizer.save_pretrained(final_training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_labels = test_results['true_labels']\n",
    "test_predictions = test_results['predictions']\n",
    "\n",
    "errors = [(i, pred, true) for i, (pred, true) in \n",
    "         enumerate(zip(test_predictions, test_true_labels)) if pred != true]\n",
    "\n",
    "print(f\"\\nTotal errors: {len(errors)} out of {len(test_predictions)} samples ({len(errors)/len(test_predictions)*100:.2f}%)\")\n",
    "\n",
    "for i, (idx, pred, true) in enumerate(errors[:5]):\n",
    "    actual_idx = X_test.index[idx]  # Get the original index in the dataset\n",
    "    text = X_test.iloc[idx][:500] + \"...\"  # Show first 500 chars\n",
    "    \n",
    "    print(f\"\\nError {i+1}:\")\n",
    "    print(f\"Predicted: {'hyperpartisan' if pred == 1 else 'not hyperpartisan'}\")\n",
    "    print(f\"Actual: {'hyperpartisan' if true == 1 else 'not hyperpartisan'}\")\n",
    "    print(f\"Text snippet: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hyperpartisan(text, model=final_model, tokenizer=tokenizer):\n",
    "    \"\"\"Predict if a text is hyperpartisan\"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=config[\"data\"][\"max_length\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "        confidence = probs[0][prediction].item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'hyperpartisan' if prediction == 1 else 'not hyperpartisan',\n",
    "        'confidence': confidence,\n",
    "        'label': prediction\n",
    "    }\n",
    "\n",
    "sample_texts = [\n",
    "    X_test.iloc[0],  \n",
    "    \"This article proves that the President is the worst in history and a complete disaster for America.\",  # Likely hyperpartisan\n",
    "    \"The Senate voted yesterday on the new healthcare bill, with 45 votes for and 55 against.\"  # Likely not hyperpartisan\n",
    "]\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    result = predict_hyperpartisan(text)\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Prediction: {result['prediction']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
