{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.utils import load_cleaned_data, split, compute_metrics, load_config\n",
    "from transformers import (\n",
    "    XLMRobertaTokenizer, \n",
    "    XLMRobertaForSequenceClassification,\n",
    "    XLMRobertaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    "    )\n",
    "import numpy as np\n",
    "import torch\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset as HFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 645\n",
      "Label distribution: {0: 407, 1: 238}\n",
      "\n",
      "Sample article (non-hyperpartisan):\n",
      "It's 1968 All Over Again. Almost a half-century ago, in 1968, the United States seemed to be falling apart. The Vietnam War, a bitter and close presidential election, antiwar protests, racial riots, p...\n",
      "\n",
      "Sample article (hyperpartisan):\n",
      "Kucinich: Reclaiming the money power. Money ( Image by 401(K) 2013 ) Permission Details DMCA No Pill Can Stop Tinnitus, But This 1 Weird Trick Can The walls are closing in on Congress. Terrifying wall...\n"
     ]
    }
   ],
   "source": [
    "config = load_config(\"../cfg/xlm_roberta.json\")\n",
    "# Load data from your existing code\n",
    "data = load_cleaned_data(config[\"data\"][\"preprocessed_data_path\"])\n",
    "X = data['full_text']\n",
    "y = data['label']\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Label distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nSample article (non-hyperpartisan):\")\n",
    "print(X[y == 0].iloc[0][:200] + \"...\")\n",
    "print(\"\\nSample article (hyperpartisan):\")\n",
    "print(X[y == 1].iloc[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model name and tokenizer from config\n",
    "model_name = config[\"model\"][\"base_model\"]\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set up XLM-RoBERTa configuration from config\n",
    "xlm_roberta_config = XLMRobertaConfig.from_pretrained(model_name)\n",
    "xlm_roberta_config.classifier_dropout = config[\"model\"][\"classifier_dropout\"]\n",
    "xlm_roberta_config.num_labels = config[\"model\"][\"num_labels\"]\n",
    "\n",
    "def tokenize_texts(texts, labels, tokenizer, max_length=config[\"data\"][\"max_length\"]):\n",
    "    \"\"\"Tokenize texts and prepare for model input\"\"\"\n",
    "    dataset_dict = {'text': texts.tolist(), 'label': labels.tolist()}\n",
    "    dataset = HFDataset.from_dict(dataset_dict)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    # Map tokenization function\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=['text']\n",
    "    )\n",
    "    \n",
    "    # Format for PyTorch\n",
    "    tokenized_dataset.set_format(\n",
    "        type='torch', \n",
    "        columns=['input_ids', 'attention_mask', 'label']\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU.\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "Original training data distribution: {0: 243, 1: 169}\n",
      "Resampled training data distribution: {1: 243, 0: 243}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1854ead57e6648719eec946327f5e531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adafa1c207e4aca8ee9b2b9e3384455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/104 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tinnitussen/DAT550/DAT550-2025---Hyperpartisan-News-Detection-/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_25728/1400868608.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 38/248 00:39 < 03:52, 0.90 it/s, Epoch 1.19/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.676359</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split(X, y)\n",
    "# Cross-validation setup from config\n",
    "n_splits = config[\"cross_validation\"][\"n_splits\"]\n",
    "kf = KFold(\n",
    "    n_splits=n_splits, \n",
    "    shuffle=config[\"cross_validation\"][\"shuffle\"], \n",
    "    random_state=config[\"cross_validation\"][\"random_state\"]\n",
    ")\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)): # Important: Use X_train for cross-validation, not X\n",
    "    print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Apply class balancing if configured\n",
    "    if config[\"class_balancing\"][\"use_balancing\"]:\n",
    "        if config[\"class_balancing\"][\"method\"] == \"oversample\":\n",
    "            ros = RandomOverSampler(random_state=config[\"cross_validation\"][\"random_state\"])\n",
    "            train_indices = np.array(range(len(X_train_fold))).reshape(-1, 1)\n",
    "            train_indices_resampled, y_train_resampled = ros.fit_resample(train_indices, y_train_fold.values)\n",
    "            train_indices_resampled = train_indices_resampled.flatten()\n",
    "            X_train_resampled = X_train_fold.iloc[train_indices_resampled].reset_index(drop=True)\n",
    "            y_train_resampled = pd.Series(y_train_resampled)\n",
    "            \n",
    "            print(f\"Original training data distribution: {y_train_fold.value_counts().to_dict()}\")\n",
    "            print(f\"Resampled training data distribution: {y_train_resampled.value_counts().to_dict()}\")\n",
    "    else:\n",
    "        # No resampling\n",
    "        X_train_resampled, y_train_resampled = X_train_fold, y_train_fold\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_dataset = tokenize_texts(\n",
    "        X_train_resampled, \n",
    "        y_train_resampled, \n",
    "        tokenizer, \n",
    "        max_length=config[\"data\"][\"max_length\"]\n",
    "    )\n",
    "    val_dataset = tokenize_texts(\n",
    "        X_val_fold, \n",
    "        y_val_fold, \n",
    "        tokenizer, \n",
    "        max_length=config[\"data\"][\"max_length\"]\n",
    "    )\n",
    "    \n",
    "    # Create a fresh model for this fold\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=xlm_roberta_config\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Training arguments from config\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{config['training']['output_dir']}/xlm_roberta_fold_{fold+1}\",\n",
    "        learning_rate=config[\"training\"][\"learning_rate\"],\n",
    "        per_device_train_batch_size=config[\"training\"][\"batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"training\"][\"eval_batch_size\"],\n",
    "        num_train_epochs=config[\"training\"][\"epochs\"],\n",
    "        weight_decay=config[\"training\"][\"weight_decay\"],\n",
    "        evaluation_strategy=config[\"training\"][\"evaluation_strategy\"],\n",
    "        save_strategy=config[\"training\"][\"save_strategy\"],\n",
    "        load_best_model_at_end=config[\"training\"][\"load_best_model_at_end\"],\n",
    "        metric_for_best_model=config[\"training\"][\"metric_for_best_model\"],\n",
    "        save_total_limit=config[\"training\"][\"save_total_limit\"],\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=config[\"training\"][\"early_stopping_patience\"])\n",
    "            ]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    results = trainer.evaluate()\n",
    "    fold_results.append(results)\n",
    "    \n",
    "    # Save model\n",
    "    trainer.save_model(f\"{training_args.output_dir}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'balanced_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Analyze cross-validation results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m accuracies = [\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbalanced_accuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m fold_results]\n\u001b[32m      3\u001b[39m precisions = [result[\u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m fold_results]\n\u001b[32m      4\u001b[39m recalls = [result[\u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m fold_results]\n",
      "\u001b[31mKeyError\u001b[39m: 'balanced_accuracy'"
     ]
    }
   ],
   "source": [
    "# Analyze cross-validation results\n",
    "accuracies = [result['balanced_accuracy'] for result in fold_results]\n",
    "precisions = [result['precision'] for result in fold_results]\n",
    "recalls = [result['recall'] for result in fold_results]\n",
    "f1_scores = [result['f1'] for result in fold_results]\n",
    "\n",
    "print(\"\\nCross-validation summary:\")\n",
    "print(f\"Balanced accuracy: {np.mean(accuracies):.4f} Â± {np.std(accuracies):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f} Â± {np.std(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f} Â± {np.std(recalls):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}\")\n",
    "\n",
    "# Visualize metrics across folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "fold_nums = list(range(1, n_splits + 1))\n",
    "plt.plot(fold_nums, accuracies, 'o-', label='Balanced accuracy')\n",
    "plt.plot(fold_nums, precisions, 'o-', label='Precision')\n",
    "plt.plot(fold_nums, recalls, 'o-', label='Recall')\n",
    "plt.plot(fold_nums, f1_scores, 'o-', label='F1 Score')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Across Folds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "train_dataset = tokenize_texts(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    tokenizer, \n",
    "    max_length=config[\"data\"][\"max_length\"]\n",
    ")\n",
    "test_dataset = tokenize_texts(\n",
    "    X_test, \n",
    "    y_test, \n",
    "    tokenizer, \n",
    "    max_length=config[\"data\"][\"max_length\"]\n",
    ")\n",
    "\n",
    "# Create final model\n",
    "final_model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=xlm_roberta_config\n",
    ").to(device)\n",
    "\n",
    "# Training arguments for final model\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=f\"{config['training']['output_dir']}/xlm_roberta_final\",\n",
    "    learning_rate=config[\"training\"][\"learning_rate\"],\n",
    "    per_device_train_batch_size=config[\"training\"][\"batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"training\"][\"eval_batch_size\"],\n",
    "    num_train_epochs=config[\"training\"][\"epochs\"],\n",
    "    weight_decay=config[\"training\"][\"weight_decay\"],\n",
    "    evaluation_strategy=config[\"training\"][\"evaluation_strategy\"],\n",
    "    save_strategy=config[\"training\"][\"save_strategy\"],\n",
    "    load_best_model_at_end=config[\"training\"][\"load_best_model_at_end\"],\n",
    "    metric_for_best_model=config[\"training\"][\"metric_for_best_model\"],\n",
    "    save_total_limit=config[\"training\"][\"save_total_limit\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=config[\"training\"][\"early_stopping_patience\"])]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "final_trainer.train()\n",
    "\n",
    "# Evaluate and save the model\n",
    "test_results = final_trainer.evaluate()\n",
    "print(\"\\nTest set evaluation:\")\n",
    "print(test_results)\n",
    "\n",
    "# Save the final model\n",
    "final_trainer.save_model(final_training_args.output_dir)\n",
    "tokenizer.save_pretrained(final_training_args.output_dir)# Train final model using the original train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_labels = test_results['true_labels']\n",
    "test_predictions = test_results['predictions']\n",
    "\n",
    "errors = [(i, pred, true) for i, (pred, true) in \n",
    "         enumerate(zip(test_predictions, test_true_labels)) if pred != true]\n",
    "\n",
    "print(f\"\\nTotal errors: {len(errors)} out of {len(test_predictions)} samples ({len(errors)/len(test_predictions)*100:.2f}%)\")\n",
    "\n",
    "for i, (idx, pred, true) in enumerate(errors[:5]):\n",
    "    actual_idx = X_test.index[idx]  # Get the original index in the dataset\n",
    "    text = X_test.iloc[idx][:500] + \"...\"  # Show first 500 chars\n",
    "    \n",
    "    print(f\"\\nError {i+1}:\")\n",
    "    print(f\"Predicted: {'hyperpartisan' if pred == 1 else 'not hyperpartisan'}\")\n",
    "    print(f\"Actual: {'hyperpartisan' if true == 1 else 'not hyperpartisan'}\")\n",
    "    print(f\"Text snippet: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hyperpartisan(text, model=final_model, tokenizer=tokenizer):\n",
    "    \"\"\"Predict if a text is hyperpartisan\"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=config[\"data\"][\"max_length\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "        confidence = probs[0][prediction].item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'hyperpartisan' if prediction == 1 else 'not hyperpartisan',\n",
    "        'confidence': confidence,\n",
    "        'label': prediction\n",
    "    }\n",
    "\n",
    "sample_texts = [\n",
    "    X_test.iloc[0],  \n",
    "    \"This article proves that the President is the worst in history and a complete disaster for America.\",  # Likely hyperpartisan\n",
    "    \"The Senate voted yesterday on the new healthcare bill, with 45 votes for and 55 against.\"  # Likely not hyperpartisan\n",
    "]\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    result = predict_hyperpartisan(text)\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Prediction: {result['prediction']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
