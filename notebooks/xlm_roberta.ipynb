{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # add parent directory to path\n",
    "from utils.utils import load_cleaned_data, metrics, split\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's load and explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 645\n",
      "Label distribution: {0: 407, 1: 238}\n",
      "\n",
      "Sample article (non-hyperpartisan):\n",
      "It's 1968 All Over Again. Almost a half-century ago, in 1968, the United States seemed to be falling apart. The Vietnam War, a bitter and close presidential election, antiwar protests, racial riots, p...\n",
      "\n",
      "Sample article (hyperpartisan):\n",
      "Kucinich: Reclaiming the money power. Money ( Image by 401(K) 2013 ) Permission Details DMCA No Pill Can Stop Tinnitus, But This 1 Weird Trick Can The walls are closing in on Congress. Terrifying wall...\n"
     ]
    }
   ],
   "source": [
    "# Load data from your existing code\n",
    "data = load_cleaned_data(\"../data/preprocessed/byarticle_clean.tsv\")\n",
    "X = data['full_text']\n",
    "y = data['label']\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Label distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nSample article (non-hyperpartisan):\")\n",
    "print(X[y == 0].iloc[0][:200] + \"...\")\n",
    "print(\"\\nSample article (hyperpartisan):\")\n",
    "print(X[y == 1].iloc[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's set up the tokenizer and create a custom dataset class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([512])\n",
      "Attention mask shape: torch.Size([512])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"xlm-roberta-base\"  # or \"xlm-roberta-large\" for better performance\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "#model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "#    model_name,\n",
    "#    num_labels=2,\n",
    "#    #classifier_dropout=0.2\n",
    "#)\n",
    "\n",
    "class HyperpartisanDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = int(self.labels.iloc[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "# Test the dataset class with a small sample\n",
    "test_dataset = HyperpartisanDataset(X.iloc[:5], y.iloc[:5], tokenizer)\n",
    "sample_item = test_dataset[0]\n",
    "print(f\"Input shape: {sample_item['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {sample_item['attention_mask'].shape}\")\n",
    "print(f\"Label: {sample_item['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's define our training and evaluation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU.\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Evaluating\")\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='binary', zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, average='binary', zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, average='binary', zero_division=0)\n",
    "\n",
    "    unique_true, counts_true = np.unique(true_labels, return_counts=True)\n",
    "    unique_pred, counts_pred = np.unique(predictions, return_counts=True)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"True labels distribution: {dict(zip(unique_true, counts_true))}\")\n",
    "    print(f\"Predicted labels distribution: {dict(zip(unique_pred, counts_pred))}\")\n",
    "    \n",
    "    return {\n",
    "        'true_labels': true_labels,\n",
    "        'predictions': predictions,\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "Training samples: 516\n",
      "Validation samples: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726b9543134945219aa969484bbaab10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6522\n",
      "Validation results:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe4ab481a234a9db907d41f02a92cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7135\n",
      "Accuracy: 0.6279\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "True labels distribution: {np.int64(0): np.int64(81), np.int64(1): np.int64(48)}\n",
      "Predicted labels distribution: {np.int64(0): np.int64(129)}\n",
      "\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc98747cfd2443cb0ae3af8d7208865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6108\n",
      "Validation results:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf88c2e1c2f403b8fb67caf36938e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5610\n",
      "Accuracy: 0.7287\n",
      "Precision: 0.5942\n",
      "Recall: 0.8542\n",
      "F1 Score: 0.7009\n",
      "True labels distribution: {np.int64(0): np.int64(81), np.int64(1): np.int64(48)}\n",
      "Predicted labels distribution: {np.int64(0): np.int64(60), np.int64(1): np.int64(69)}\n",
      "\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7caf4b1cc49b4b06a9d1a39c25e3f89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5064\n",
      "Validation results:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7cd51707aa04d6884ec01e94fd27201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4481\n",
      "Accuracy: 0.7597\n",
      "Precision: 0.6667\n",
      "Recall: 0.7083\n",
      "F1 Score: 0.6869\n",
      "True labels distribution: {np.int64(0): np.int64(81), np.int64(1): np.int64(48)}\n",
      "Predicted labels distribution: {np.int64(0): np.int64(78), np.int64(1): np.int64(51)}\n",
      "\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f9f654650649d0a738d13743b7403b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3804\n",
      "Validation results:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb3ca60d75741a39ffcec3918db074b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5353\n",
      "Accuracy: 0.7907\n",
      "Precision: 0.7692\n",
      "Recall: 0.6250\n",
      "F1 Score: 0.6897\n",
      "True labels distribution: {np.int64(0): np.int64(81), np.int64(1): np.int64(48)}\n",
      "Predicted labels distribution: {np.int64(0): np.int64(90), np.int64(1): np.int64(39)}\n",
      "\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2f6c8dc0684e9da829107cc5b518d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train_fold)}\")\n",
    "    print(f\"Validation samples: {len(X_val_fold)}\")\n",
    "    \n",
    "    train_dataset = HyperpartisanDataset(X_train_fold, y_train_fold, tokenizer)\n",
    "    val_dataset = HyperpartisanDataset(X_val_fold, y_val_fold, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        \n",
    "        print(\"Validation results:\")\n",
    "        val_results = evaluate(model, val_loader, device)\n",
    "    \n",
    "    fold_results.append(val_results)\n",
    "    \n",
    "    model.save_pretrained(f\"../models/xlm_roberta_fold_{fold+1}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's analyze the cross-validation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cross-validation results\n",
    "accuracies = [result['accuracy'] for result in fold_results]\n",
    "precisions = [result['precision'] for result in fold_results]\n",
    "recalls = [result['recall'] for result in fold_results]\n",
    "f1_scores = [result['f1'] for result in fold_results]\n",
    "\n",
    "print(\"\\nCross-validation summary:\")\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "\n",
    "# Visualize metrics across folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "fold_nums = list(range(1, n_splits + 1))\n",
    "plt.plot(fold_nums, accuracies, 'o-', label='Accuracy')\n",
    "plt.plot(fold_nums, precisions, 'o-', label='Precision')\n",
    "plt.plot(fold_nums, recalls, 'o-', label='Recall')\n",
    "plt.plot(fold_nums, f1_scores, 'o-', label='F1 Score')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Across Folds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's train our final model on the original train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split(X, y)\n",
    "\n",
    "train_dataset = HyperpartisanDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = HyperpartisanDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "final_model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# Prepare optimizer and scheduler\n",
    "optimizer = AdamW(final_model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nTraining final model...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(final_model, train_loader, optimizer, scheduler, device)\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "final_model.save_pretrained(\"../models/xlm_roberta_final\")\n",
    "tokenizer.save_pretrained(\"../models/xlm_roberta_final\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nTest set evaluation:\")\n",
    "test_results = evaluate(final_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the most challenging examples - the ones where our model made mistakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors\n",
    "test_true_labels = test_results['true_labels']\n",
    "test_predictions = test_results['predictions']\n",
    "\n",
    "errors = [(i, pred, true) for i, (pred, true) in \n",
    "          enumerate(zip(test_predictions, test_true_labels)) if pred != true]\n",
    "\n",
    "print(f\"\\nTotal errors: {len(errors)} out of {len(test_predictions)} samples ({len(errors)/len(test_predictions)*100:.2f}%)\")\n",
    "\n",
    "# Examine a few errors\n",
    "for i, (idx, pred, true) in enumerate(errors[:5]):\n",
    "    actual_idx = X_test.index[idx]  # Get the original index in the dataset\n",
    "    text = X_test.iloc[idx][:500] + \"...\"  # Show first 500 chars\n",
    "    \n",
    "    print(f\"\\nError {i+1}:\")\n",
    "    print(f\"Predicted: {'hyperpartisan' if pred == 1 else 'not hyperpartisan'}\")\n",
    "    print(f\"Actual: {'hyperpartisan' if true == 1 else 'not hyperpartisan'}\")\n",
    "    print(f\"Text snippet: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, let's implement a simple prediction function for new articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hyperpartisan(text, model, tokenizer, device, max_length=1024):\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, prediction = torch.max(outputs.logits, dim=1)\n",
    "    \n",
    "    result = prediction.item()\n",
    "    confidence = torch.softmax(outputs.logits, dim=1)[0][result].item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'hyperpartisan' if result == 1 else 'not hyperpartisan',\n",
    "        'confidence': confidence,\n",
    "        'label': result\n",
    "    }\n",
    "\n",
    "# Test with a couple of examples\n",
    "sample_texts = [\n",
    "    X_test.iloc[0],  # Test with a real example\n",
    "    \"This article proves that the President is the worst in history and a complete disaster for America.\",  # Likely hyperpartisan\n",
    "    \"The Senate voted yesterday on the new healthcare bill, with 45 votes for and 55 against.\"  # Likely not hyperpartisan\n",
    "]\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    result = predict_hyperpartisan(text, final_model, tokenizer, device)\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Prediction: {result['prediction']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
