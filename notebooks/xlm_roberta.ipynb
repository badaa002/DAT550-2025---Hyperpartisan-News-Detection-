{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 12:49:18.205149: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-07 12:49:18.472443: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744022958.568324    2088 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744022958.596746    2088 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744022958.819213    2088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744022958.819264    2088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744022958.819265    2088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744022958.819267    2088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-07 12:49:18.843595: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.utils import load_cleaned_data, compute_metrics, load_config, plot_confusion_matrix, metrics, apply_oversampling, store_metrics\n",
    "from transformers import (\n",
    "    XLMRobertaTokenizerFast, \n",
    "    XLMRobertaForSequenceClassification,\n",
    "    XLMRobertaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    "    )\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datasets import Dataset as HFDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 645\n",
      "Label distribution: {0: 407, 1: 238}\n",
      "\n",
      "Sample article (non-hyperpartisan):\n",
      "It's 1968 All Over Again. Almost a half-century ago, in 1968, the United States seemed to be falling apart. The Vietnam War, a bitter and close presidential election, antiwar protests, racial riots, p...\n",
      "\n",
      "Sample article (hyperpartisan):\n",
      "Kucinich: Reclaiming the money power. Money ( Image by 401(K) 2013 ) Permission Details DMCA No Pill Can Stop Tinnitus, But This 1 Weird Trick Can The walls are closing in on Congress. Terrifying wall...\n"
     ]
    }
   ],
   "source": [
    "run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_id = f\"run_{run_timestamp}\"\n",
    "config = load_config(\"../cfg/xlm_roberta.json\")\n",
    "\n",
    "# Set seeds\n",
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(config[\"seed\"]) # Transformers\n",
    "\n",
    "data = load_cleaned_data(config[\"data\"][\"preprocessed_data_path\"])\n",
    "X = data[\"full_text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Label distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nSample article (non-hyperpartisan):\")\n",
    "print(X[y == 0].iloc[0][:200] + \"...\")\n",
    "print(\"\\nSample article (hyperpartisan):\")\n",
    "print(X[y == 1].iloc[0][:200] + \"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2031 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with more than 512 tokens: 376\n"
     ]
    }
   ],
   "source": [
    "# Set up model name and tokenizer from config\n",
    "model_name = config[\"model\"][\"base_model\"]\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Print the number of articles with more than 512 tokens\n",
    "def count_tokens(article):\n",
    "    return len(tokenizer(article)[\"input_ids\"])\n",
    "long_articles = X[X.apply(count_tokens) > 512]\n",
    "print(f\"Number of articles with more than 512 tokens: {len(long_articles)}\")\n",
    "\n",
    "# Set up XLM-RoBERTa configuration from config\n",
    "xlm_roberta_config = XLMRobertaConfig.from_pretrained(model_name)\n",
    "xlm_roberta_config.classifier_dropout = config[\"model\"][\"classifier_dropout\"]\n",
    "xlm_roberta_config.num_labels = config[\"model\"][\"num_labels\"]\n",
    "\n",
    "\n",
    "def tokenize_texts(texts, labels, tokenizer, is_training=True):\n",
    "    \"\"\"Tokenize texts with sliding window only during training\"\"\"\n",
    "    dataset_dict = {\"text\": texts.tolist(), \"label\": labels.tolist()}\n",
    "    dataset = HFDataset.from_dict(dataset_dict)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        # Only use sliding window during training\n",
    "        if is_training:\n",
    "            tokenized = tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=config[\"tokenizer\"][\"padding\"],\n",
    "                truncation=config[\"tokenizer\"][\"truncation\"],\n",
    "                max_length=config[\"data\"][\"max_length\"],\n",
    "                add_special_tokens=config[\"tokenizer\"][\"add_special_tokens\"],\n",
    "                return_tensors=config[\"tokenizer\"][\"return_tensors\"],\n",
    "                stride=config[\"tokenizer\"][\"stride\"],\n",
    "                return_overflowing_tokens=config[\"tokenizer\"][\"return_overflowing_tokens\"],\n",
    "            )\n",
    "            \n",
    "            # Map labels to chunks\n",
    "            if \"overflow_to_sample_mapping\" in tokenized:\n",
    "                sample_map = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "                labels = []\n",
    "                for i in sample_map:\n",
    "                    labels.append(examples[\"label\"][i])\n",
    "                return {**tokenized, \"label\": labels}\n",
    "        else:\n",
    "            # Standard tokenization for prediction\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=config[\"tokenizer\"][\"padding\"],\n",
    "                truncation=config[\"tokenizer\"][\"truncation\"],\n",
    "                max_length=config[\"data\"][\"max_length\"],\n",
    "                add_special_tokens=config[\"tokenizer\"][\"add_special_tokens\"],\n",
    "                return_tensors=config[\"tokenizer\"][\"return_tensors\"]\n",
    "            )\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Map tokenization function\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=config[\"tokenizer\"][\"batch_process\"], \n",
    "        remove_columns=config[\"tokenizer\"][\"remove_columns\"]\n",
    "    )\n",
    "    \n",
    "    # Format for PyTorch\n",
    "    tokenized_dataset.set_format(\n",
    "        type=config[\"tokenizer\"][\"format_type\"], \n",
    "        columns=config[\"tokenizer\"][\"format_columns\"]\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU.\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0d1701dbf14a7098494746cbb36a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333cd2a820244f29be2454136be73f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tinnitussen/DAT550/DAT550-2025---Hyperpartisan-News-Detection-/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2088/854625038.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='381' max='2580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 381/2580 02:02 < 11:49, 3.10 it/s, Epoch 2.21/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.500113</td>\n",
       "      <td>0.755081</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.682927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.542079</td>\n",
       "      <td>0.799797</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.745763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cross-validation setup from config\n",
    "n_splits = config[\"cross_validation\"][\"n_splits\"]\n",
    "kf = StratifiedKFold(\n",
    "    n_splits=n_splits, \n",
    "    shuffle=config[\"cross_validation\"][\"shuffle\"], \n",
    "    random_state=config[\"cross_validation\"][\"random_state\"]\n",
    ")\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Apply class balancing if configured\n",
    "    if config[\"class_balancing\"][\"use_balancing\"]:\n",
    "        X_train_fold, y_train_fold = apply_oversampling(\n",
    "            X_train_fold, \n",
    "            y_train_fold, \n",
    "            config[\"class_balancing\"][\"oversampling_method\"]\n",
    "        )\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_dataset = tokenize_texts(\n",
    "        X_train_fold, \n",
    "        y_train_fold, \n",
    "        tokenizer,\n",
    "        is_training=True\n",
    "    )\n",
    "    val_dataset = tokenize_texts(\n",
    "        X_val_fold, \n",
    "        y_val_fold, \n",
    "        tokenizer, \n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    # Create a fresh model for this fold\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=xlm_roberta_config\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Training arguments from config\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{config[\"training\"][\"output_dir\"]}/xlm_roberta_fold_{fold+1}\",\n",
    "        learning_rate=config[\"training\"][\"learning_rate\"],\n",
    "        per_device_train_batch_size=config[\"training\"][\"batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"training\"][\"eval_batch_size\"],\n",
    "        num_train_epochs=config[\"training\"][\"epochs\"],\n",
    "        weight_decay=config[\"training\"][\"weight_decay\"],\n",
    "        evaluation_strategy=config[\"training\"][\"evaluation_strategy\"],\n",
    "        save_strategy=config[\"training\"][\"save_strategy\"],\n",
    "        load_best_model_at_end=config[\"training\"][\"load_best_model_at_end\"],\n",
    "        metric_for_best_model=config[\"training\"][\"metric_for_best_model\"],\n",
    "        save_total_limit=config[\"training\"][\"save_total_limit\"],\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=config[\"training\"][\"early_stopping_patience\"])\n",
    "            ]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    results = trainer.evaluate()\n",
    "    fold_results.append(results)\n",
    "    \n",
    "    # Save model\n",
    "    model_base_path = config['training']['output_dir']\n",
    "    model_path = f\"{model_base_path}/{run_id}/xlm_roberta_fold_{fold+1}\"\n",
    "    trainer.save_model(model_path)\n",
    "    print(f\"Saved best model from fold {fold+1} (best F1: {max([x['eval_f1'] for x in trainer.state.log_history if 'eval_f1' in x])})\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fold_results)\n",
    "# Analyze cross-validation results\n",
    "accuracies = [result[\"eval_balanced_accuracy\"] for result in fold_results]\n",
    "precisions = [result[\"eval_precision\"] for result in fold_results]\n",
    "recalls = [result[\"eval_recall\"] for result in fold_results]\n",
    "f1_scores = [result[\"eval_f1\"] for result in fold_results]\n",
    "\n",
    "print(\"\\nCross-validation summary:\")\n",
    "print(f\"Balanced accuracy: {np.mean(accuracies):.4f} Â± {np.std(accuracies):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f} Â± {np.std(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f} Â± {np.std(recalls):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}\")\n",
    "\n",
    "# Visualize metrics across folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "fold_nums = list(range(1, n_splits + 1))\n",
    "plt.plot(fold_nums, accuracies, \"o-\", label=\"Balanced accuracy\")\n",
    "plt.plot(fold_nums, precisions, \"o-\", label=\"Precision\")\n",
    "plt.plot(fold_nums, recalls, \"o-\", label=\"Recall\")\n",
    "plt.plot(fold_nums, f1_scores, \"o-\", label=\"F1 Score\")\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Model Performance Across Folds\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble predictions on the test set for robustness\n",
    "fold_metrics = [result['eval_f1'] for result in fold_results]\n",
    "\n",
    "fold_models = []\n",
    "for fold in range(config[\"cross_validation\"][\"n_splits\"]):\n",
    "    model_base_path = config['training']['output_dir']\n",
    "    model_path = f\"{model_base_path}/{run_id}/xlm_roberta_fold_{fold+1}\"\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        model_path\n",
    "    ).to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    fold_models.append(model)\n",
    "\n",
    "def ensemble_predict(inputs, fold_models, fold_metrics, device):\n",
    "    \"\"\"Make prediction using weighted ensemble of models\"\"\"\n",
    "    # Calculate weights from metrics\n",
    "    weights = torch.tensor(fold_metrics, device=device)\n",
    "    weights = weights / weights.sum()  # Normalize\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    all_probs = []\n",
    "    for model in fold_models:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    # Apply weighted average\n",
    "    weighted_probs = torch.zeros_like(all_probs[0])\n",
    "    for i, probs in enumerate(all_probs):\n",
    "        weighted_probs += probs * weights[i]\n",
    "    \n",
    "    return weighted_probs\n",
    "\n",
    "# Get predictions for all test samples\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "for text in #ADD YOUR TEST DATA HERE#:\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=config[\"data\"][\"max_length\"],\n",
    "        truncation=config[\"tokenizer\"][\"truncation\"],\n",
    "        padding=config[\"tokenizer\"][\"padding\"],\n",
    "        add_special_tokens=config[\"tokenizer\"][\"add_special_tokens\"],\n",
    "        return_tensors=config[\"tokenizer\"][\"return_tensors\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get ensemble prediction\n",
    "    weighted_probs = ensemble_predict(inputs, fold_models, fold_metrics, device)\n",
    "    \n",
    "    # Extract prediction and confidence\n",
    "    pred_label = torch.argmax(weighted_probs, dim=1).item()\n",
    "    confidence = weighted_probs[0][1].item()  # Probability of class 1 (hyperpartisan)\n",
    "    \n",
    "    y_pred.append(pred_label)\n",
    "    y_pred_proba.append(confidence)\n",
    "\n",
    "# Convert to numpy arrays for the metrics function\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(\"\\nEnsemble Model Evaluation:\")\n",
    "metrics(y, y_pred, y_pred_proba)\n",
    "plot_confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "store_metrics(\n",
    "    results_list=results_list,\n",
    "    model_name=\"XLM-RoBERTa\",\n",
    "    setup_label=\"WeightedEnsemble+SlidingWindow\",\n",
    "    y_test=y,\n",
    "    y_pred=y_pred,\n",
    "    y_pred_proba=y_pred_proba\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "hyperparams = {\n",
    "    # Model parameters\n",
    "    \"base_model\": config[\"model\"][\"base_model\"],\n",
    "    \"classifier_dropout\": config[\"model\"][\"classifier_dropout\"],\n",
    "    \n",
    "    # Training parameters\n",
    "    \"learning_rate\": config[\"training\"][\"learning_rate\"],\n",
    "    \"batch_size\": config[\"training\"][\"batch_size\"],\n",
    "    \"eval_batch_size\": config[\"training\"][\"eval_batch_size\"],\n",
    "    \"epochs\": config[\"training\"][\"epochs\"],\n",
    "    \"weight_decay\": config[\"training\"][\"weight_decay\"],\n",
    "    \"evaluation_strategy\": config[\"training\"][\"evaluation_strategy\"],\n",
    "    \"save_strategy\": config[\"training\"][\"save_strategy\"],\n",
    "    \"load_best_model_at_end\": config[\"training\"][\"load_best_model_at_end\"],\n",
    "    \"metric_for_best_model\": config[\"training\"][\"metric_for_best_model\"],\n",
    "    \"save_total_limit\": config[\"training\"][\"save_total_limit\"],\n",
    "    \"early_stopping_patience\": config[\"training\"][\"early_stopping_patience\"],\n",
    "    \"use_fp16\": config[\"training\"][\"use_fp16\"],\n",
    "    \n",
    "    # Cross-validation settings\n",
    "    \"n_splits\": config[\"cross_validation\"][\"n_splits\"],\n",
    "    \"shuffle\": config[\"cross_validation\"][\"shuffle\"],\n",
    "    \n",
    "    # Data parameters\n",
    "    \"max_length\": config[\"data\"][\"max_length\"],\n",
    "    \"test_size\": config[\"data\"][\"test_size\"],\n",
    "    \"data_random_state\": config[\"data\"][\"random_state\"],\n",
    "    \n",
    "    # Class balancing\n",
    "    \"use_balancing\": config[\"class_balancing\"][\"use_balancing\"],\n",
    "    \"balancing_method\": config[\"class_balancing\"][\"method\"],\n",
    "    \n",
    "    # Tokenizer settings\n",
    "    \"padding\": config[\"tokenizer\"][\"padding\"],\n",
    "    \"truncation\": config[\"tokenizer\"][\"truncation\"],\n",
    "    \"add_special_tokens\": config[\"tokenizer\"][\"add_special_tokens\"],\n",
    "    \"batch_process\": config[\"tokenizer\"][\"batch_process\"],\n",
    "    \"stride\": config[\"tokenizer\"][\"stride\"],\n",
    "    \"return_overflowing_tokens\": config[\"tokenizer\"][\"return_overflowing_tokens\"],\n",
    "}\n",
    "\n",
    "# Add all hyperparameters as columns in one operation\n",
    "for param_name, param_value in hyperparams.items():\n",
    "    results_df[param_name] = param_value\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "try:\n",
    "    existing_df = pd.read_csv(\"../results/byarticle_advanced_results.csv\")\n",
    "    combined_df = pd.concat([existing_df, results_df], ignore_index=True)\n",
    "    combined_df.to_csv(\"../results/byarticle_advanced_results.csv\", index=False)\n",
    "except FileNotFoundError:\n",
    "    results_df.to_csv(\"../results/byarticle_advanced_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
