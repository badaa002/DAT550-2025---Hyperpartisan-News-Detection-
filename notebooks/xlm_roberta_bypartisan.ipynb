{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load XLM-RoBERTa Ensemble and Predict\n",
                "\n",
                "This notebook loads the best models saved during cross-validation from a specific training run (`run_id`) and uses them as a weighted ensemble to predict on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import json\n",
                "import numpy as np\n",
                "import torch\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
                "from src.utils import load_cleaned_data, load_config, metrics, plot_confusion_matrix\n",
                "from src.weighted_ensemble_predict import weighted_ensemble_predict\n",
                "\n",
                "from transformers import (\n",
                "    XLMRobertaTokenizerFast,\n",
                "    XLMRobertaForSequenceClassification,\n",
                "    XLMRobertaConfig,\n",
                "    set_seed,\n",
                ")\n",
                "\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "RUN_ID_TO_LOAD = \"\"\n",
                "CONFIG_PATH = \"../cfg/xlm_roberta.json\"\n",
                "\n",
                "config = load_config(CONFIG_PATH)\n",
                "\n",
                "set_seed(config[\"seed\"])\n",
                "torch.backends.cudnn.deterministic = True\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "if device == torch.device(\"cuda\"): torch.cuda.empty_cache()\n",
                "\n",
                "model_name = config[\"model\"][\"base_model\"]\n",
                "tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
                "print(f\"Tokenizer '{model_name}' loaded.\")\n",
                "\n",
                "xlm_roberta_config = XLMRobertaConfig.from_pretrained(model_name)\n",
                "xlm_roberta_config.classifier_dropout = config[\"model\"][\"classifier_dropout\"]\n",
                "xlm_roberta_config.num_labels = config[\"model\"][\"num_labels\"] \n",
                "\n",
                "model_base_path = config['training']['output_dir']\n",
                "run_output_dir = os.path.join(model_base_path, RUN_ID_TO_LOAD)\n",
                "metrics_path = os.path.join(run_output_dir, \"fold_metrics.json\")\n",
                "n_splits = config[\"cross_validation\"][\"n_splits\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Fold Metrics and Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fold_f1_scores = []\n",
                "with open(metrics_path, 'r') as f:\n",
                "    fold_results = json.load(f)\n",
                "metric_key = f\"eval_{config['training']['metric_for_best_model']}\"\n",
                "fold_f1_scores = [result[metric_key] for result in fold_results]\n",
                "print(f\"Loaded metrics for {len(fold_f1_scores)} folds from {metrics_path}\")\n",
                "print(f\"Fold F1 scores: {fold_f1_scores}\")\n",
                "\n",
                "fold_models = []\n",
                "print(f\"\\nLoading {n_splits} fold models...\")\n",
                "for fold in range(1, n_splits + 1):\n",
                "    model_path = os.path.join(run_output_dir, f\"xlm_roberta_fold_{fold}\")\n",
                "    print(f\"Attempting to load model from: {model_path}\")\n",
                "    try:\n",
                "        # Pass the specific config if needed, though often it's saved with the model\n",
                "        model = XLMRobertaForSequenceClassification.from_pretrained(\n",
                "            model_path,\n",
                "            config=xlm_roberta_config # Pass config just in case\n",
                "        )\n",
                "        model.to(device)\n",
                "        model.eval()  # Set to evaluation mode\n",
                "        fold_models.append(model)\n",
                "        print(f\"Loaded model for fold {fold}.\")\n",
                "    except OSError as e:\n",
                "        print(f\"Error loading model for fold {fold} from {model_path}: {e}\")\n",
                "        print(\"Ensemble will be incomplete.\")\n",
                "        # Optionally break or handle the missing model\n",
                "\n",
                "if len(fold_models) != n_splits:\n",
                "    print(f\"Warning: Expected {n_splits} models, but only loaded {len(fold_models)}. Ensemble results may be affected.\")\n",
                "    # Adjust fold_f1_scores if models are missing? Or handle in ensemble_predict?\n",
                "    # For simplicity now, we'll assume the ensemble function can handle potentially fewer models if needed."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Test Data and Perform Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_data = None\n",
                "X_test = None\n",
                "y_test = None\n",
                "\n",
                "test_data = load_cleaned_data(config[\"data\"][\"test_data_path\"])\n",
                "X_test = test_data[\"full_text\"]\n",
                "y_test = test_data[\"label\"]\n",
                "print(f\"Loaded test data: {len(X_test)} samples.\")\n",
                "\n",
                "y_pred = []\n",
                "y_pred_proba = []\n",
                "\n",
                "print(\"\\nStarting ensemble predictions on test data...\")\n",
                "for text in tqdm(X_test, desc=\"Predicting\"): \n",
                "    inputs = tokenizer(\n",
                "        text,\n",
                "        max_length=config[\"data\"][\"max_length\"],\n",
                "        truncation=config[\"tokenizer\"][\"truncation\"],\n",
                "        padding=config[\"tokenizer\"][\"padding\"], # Should match config, often 'max_length'\n",
                "        add_special_tokens=config[\"tokenizer\"][\"add_special_tokens\"],\n",
                "        return_tensors=config[\"tokenizer\"][\"return_tensors\"]\n",
                "    ).to(device)\n",
                "    \n",
                "    weighted_probs = weighted_ensemble_predict(inputs, fold_models, fold_f1_scores, device)\n",
                "    pred_label = torch.argmax(weighted_probs, dim=1).item()\n",
                "    confidence = weighted_probs[0][1].item()\n",
                "    \n",
                "    y_pred.append(pred_label)\n",
                "    y_pred_proba.append(confidence)\n",
                "    \n",
                "\n",
                "torch.cuda.empty_cache()\n",
                "y_pred = np.array(y_pred)\n",
                "y_pred_proba = np.array(y_pred_proba)\n",
                "print(\"Ensemble predictions finished.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate Ensemble Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nEnsemble Model Evaluation on Test Set:\")\n",
                "\n",
                "metrics(y_test, y_pred, y_pred_proba, print_metrics=True)\n",
                "\n",
                "plot_confusion_matrix(y_test, y_pred)\n",
                "plt.suptitle(f'Ensemble Confusion Matrix (Run ID: {RUN_ID_TO_LOAD})', y=1.02) # Add title\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
